{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"/home/wkh/XLA.jl/Project.toml\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Pkg\n",
    "Pkg.activate(\"/home/wkh/XLA.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Distributions\n",
    "Pkg.resolve()\n",
    "using Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Precompiling TensorFlow [1d978283-2c37-5f34-9a8e-e9c0ece82495]\n",
      "└ @ Base loading.jl:1186\n",
      "┌ Warning: Loading a new version of TensorFlow.jl for the first time. This initial load can take around 5 minutes as code is precompiled; subsequent usage will only take a few seconds.\n",
      "└ @ TensorFlow ~/.julia/packages/TensorFlow/eu9qM/src/TensorFlow.jl:3\n",
      "┌ Warning: Package Distributions does not have Test in its dependencies:\n",
      "│ - If you have Distributions checked out for development and have\n",
      "│   added Test as a dependency but haven't updated your primary\n",
      "│   environment's manifest file, try `Pkg.resolve()`.\n",
      "│ - Otherwise you may need to report an issue with Distributions\n",
      "└ Loading Test into Distributions from project dependency, future warnings for Distributions are suppressed.\n",
      "┌ Warning: The call to compilecache failed to create a usable precompiled cache file for TensorFlow [1d978283-2c37-5f34-9a8e-e9c0ece82495]\n",
      "│   exception = ErrorException(\"Required dependency Distributions [31c24e10-a181-5473-b8eb-7969acd0382f] failed to load from a cache file.\")\n",
      "└ @ Base loading.jl:969\n",
      "┌ Warning: Loading a new version of TensorFlow.jl for the first time. This initial load can take around 5 minutes as code is precompiled; subsequent usage will only take a few seconds.\n",
      "└ @ TensorFlow /home/wkh/.julia/packages/TensorFlow/eu9qM/src/TensorFlow.jl:3\n",
      "┌ Info: Recompiling stale cache file /home/wkh/.julia/compiled/v1.1/Distributions/xILW0.ji for Distributions [31c24e10-a181-5473-b8eb-7969acd0382f]\n",
      "└ @ Base loading.jl:1184\n",
      "┌ Warning: Package Distributions does not have Test in its dependencies:\n",
      "│ - If you have Distributions checked out for development and have\n",
      "│   added Test as a dependency but haven't updated your primary\n",
      "│   environment's manifest file, try `Pkg.resolve()`.\n",
      "│ - Otherwise you may need to report an issue with Distributions\n",
      "└ Loading Test into Distributions from project dependency, future warnings for Distributions are suppressed.\n",
      "┌ Info: Precompiling XLA [1ae4bca4-de81-11e8-0eca-6d3e4e7c4181]\n",
      "└ @ Base loading.jl:1186\n",
      "┌ Warning: Module TensorFlow with build ID 87869650927412 is missing from the cache.\n",
      "│ This may mean TensorFlow [1d978283-2c37-5f34-9a8e-e9c0ece82495] does not support precompilation but is imported by a module that does.\n",
      "└ @ Base loading.jl:947\n",
      "WARNING: could not import xla.Shape into XLA\n",
      "┌ Info: Precompiling Flux [587475ba-b771-5e3f-ad9e-33799f191a9c]\n",
      "└ @ Base loading.jl:1186\n",
      "WARNING: could not import NNlib.cudata into Tracker\n",
      "┌ Info: Precompiling Zygote [e88e6eb3-aa80-5325-afca-941959d7151f]\n",
      "└ @ Base loading.jl:1186\n"
     ]
    }
   ],
   "source": [
    "using TensorFlow, XLA, Flux, Zygote, Printf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "epoch_loop (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "include(\"/home/wkh/XLA.jl/examples/resnet50.jl\")\n",
    "include(\"/home/wkh/XLA.jl/examples/preprocessing_utils.jl\")\n",
    "include(\"/home/wkh/XLA.jl/examples/model_utils.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet50()\n",
    "tpu_model = map_to_tpu(model);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_minibatch_data (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function get_minibatch_data(::Val{batch_size}) where {batch_size}\n",
    "    # Construct HloInfeed object that will expect to receive a tuple\n",
    "    # of two arrays, one for `x` and one for `y`.  Note that incorrect sizes\n",
    "    # here will cause...unexpected results, so do your best not to do that.\n",
    "    # We feed data in as 1-dimensional UInt32 arrays, then reshape them.\n",
    "    infeed = XLA.HloInfeed(Tuple{\n",
    "        XRTArray{UInt32, (224*224*batch_size,), 1},\n",
    "        XRTArray{UInt32, (batch_size,), 1},\n",
    "    })\n",
    "\n",
    "    # Read in from the infeed\n",
    "    (x, y), _ = infeed(XLA.HloAfterAll()())\n",
    "    x = reshape(x, (224, 224, batch_size))\n",
    "    \n",
    "    # Do pixel unpacking/channel normalization.\n",
    "    x = unpack_pixels(x)\n",
    "\n",
    "    # Convert labels to (dense) onehot representation\n",
    "    y = make_onehot(y)\n",
    "    #y = convert(XRTArray{Float32}, Flux.OneHotMatrix(1000, convert(XRTArray{Int64}, y)))\n",
    "    \n",
    "    # Return our data!\n",
    "    return x, y\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "update! (generic function with 4 methods)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struct SGD\n",
    "    # Learning rate; the only data this optimizer needs to bundle with itself\n",
    "    η::XRTArray{Float32,(),0}\n",
    "end\n",
    "\n",
    "# Simplest update step in existence.\n",
    "update!(model::XRTArray, Δ::XRTArray, η) = model - (Δ .* η)\n",
    "\n",
    "# If this leaf node had no updates calculated for it, then skip out early.\n",
    "update!(model, Δ::Nothing, η) = model\n",
    "\n",
    "function update!(model, Δ, η)\n",
    "    # Base condition; if we have reached a leaf node return the inputs unchanged.\n",
    "    # Note that if `model` is an XRTArray, we will hit the override above that actually\n",
    "    # updates the model rather than this generic update!(), same for if Δ is `nothing`.\n",
    "    if nfields(model) == 0\n",
    "        return model\n",
    "    end\n",
    "    \n",
    "    # Recursively pass the fields of this model through the update machinery.  We use\n",
    "    # this strange ntuple() do-block because we cannot perform any kind of mutation\n",
    "    # (such as push!()'ing onto a list) and so we adopt this more functional-style of\n",
    "    # programming.\n",
    "    new_fields = ntuple(Val(nfields(model))) do i\n",
    "        return update!(getfield(model, i), getfield(Δ, i), η)\n",
    "    end\n",
    "    \n",
    "    # Return something of the same type as `model`, but with the new fields\n",
    "    if isa(model, Tuple)\n",
    "        return new_fields\n",
    "    else\n",
    "        return typeof(model)(new_fields...)\n",
    "    end\n",
    "end\n",
    "\n",
    "# Main entry point for this optimizer's update steps\n",
    "update!(opt::SGD, model, Δ) = update!(model, Δ, opt.η)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_loop (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define our training loop\n",
    "function train_loop(::Val{batch_size}, model, nbatches, η) where {batch_size}\n",
    "    # Initialize optimizer, will allocate space for all necessary statistics within itself\n",
    "    opt = SGD(η)\n",
    "\n",
    "    # Run until nbatches is zero\n",
    "    while nbatches > XRTArray(0)\n",
    "        # Get next minibatch of data\n",
    "        mb_data = get_minibatch_data(Val(batch_size))\n",
    "\n",
    "        # Let block to fend off the inference demons\n",
    "        loss, back = let x = mb_data[1], y = mb_data[2]\n",
    "            # Calculate forward pass to get loss, and compile backwards pass\n",
    "            # to get the updates to our model weights.\n",
    "            Zygote._forward(\n",
    "                Zygote.Context{Nothing}(nothing),\n",
    "                model -> logitcrossentropy(model(x), y),\n",
    "                model,\n",
    "            )\n",
    "        end\n",
    "\n",
    "        # Evaluate the backwards pass.  Zygote automatically calculates\n",
    "        # sensitivities upon `x` and `y`; we discard those via the tail()\n",
    "        Δ_model = Zygote.tailmemaybe(back(1f0))[1]\n",
    "\n",
    "        # Update parameters via our optimizer\n",
    "        model = update!(opt, model, Δ_model)\n",
    "\n",
    "        # Outfeed the loss\n",
    "        loss = reshape(loss, (1,))\n",
    "        XLA.HloOutfeed()((loss,), XLA.HloAfterAll()())\n",
    "\n",
    "        # Count down the batches\n",
    "        nbatches -= XRTArray(1)\n",
    "    end\n",
    "    \n",
    "    # At the end of all things, return the trained model\n",
    "    return model\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-14 16:27:03.054477: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:349] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TPUSession(<8 TPU chips in 2x2x2 topology>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XLA.initialize!(\"10.240.1.2:8470\"; reset=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Compiled training loop in 262.5 seconds\n"
     ]
    }
   ],
   "source": [
    "# Train in batch sizes of 128, for 10000 batches with a learning rate of 1e-2\n",
    "batch_size = 128\n",
    "num_batches = 10000\n",
    "η = 0.01f0\n",
    "\n",
    "# We need to work around a TensorFlow bug which doesn't choose the default infeed/outfeed\n",
    "# device placement properly.  We do so by explicitly placing all operations on the first TPU core\n",
    "sess = XLA.global_session()\n",
    "tpu_device = first(all_tpu_devices(sess))\n",
    "\n",
    "# Compile the model\n",
    "t_start = time()\n",
    "compilation_handle = @tpu_compile devices=[tpu_device] train_loop(Val(batch_size), tpu_model, XRTArray(num_batches), XRTArray(η));\n",
    "t_end = time()\n",
    "\n",
    "println(@sprintf(\"=> Compiled training loop in %.1f seconds\", t_end - t_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.1.0-DEV",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
