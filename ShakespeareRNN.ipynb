{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"/home/wkh/XLA.jl/Project.toml\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Pkg\n",
    "Pkg.activate(\"/home/wkh/XLA.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m registry at `~/.julia/registries/General`\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m git-repo `https://github.com/JuliaRegistries/General.git`\n",
      "\u001b[?25l\u001b[2K\u001b[?25h\u001b[32m\u001b[1m Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/XLA.jl/Project.toml`\n",
      " \u001b[90m [31c24e10]\u001b[39m\u001b[92m + Distributions v0.19.1\u001b[39m\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/XLA.jl/Manifest.toml`\n",
      "\u001b[90m [no changes]\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "Pkg.add(\"Distributions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Precompiling Distributions [31c24e10-a181-5473-b8eb-7969acd0382f]\n",
      "└ @ Base loading.jl:1186\n",
      "┌ Warning: Package Distributions does not have Test in its dependencies:\n",
      "│ - If you have Distributions checked out for development and have\n",
      "│   added Test as a dependency but haven't updated your primary\n",
      "│   environment's manifest file, try `Pkg.resolve()`.\n",
      "│ - Otherwise you may need to report an issue with Distributions\n",
      "└ Loading Test into Distributions from project dependency, future warnings for Distributions are suppressed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/XLA.jl/Project.toml`\n",
      "\u001b[90m [no changes]\u001b[39m\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/XLA.jl/Manifest.toml`\n",
      "\u001b[90m [no changes]\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "using Distributions\n",
    "Pkg.resolve()\n",
    "using Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Precompiling TensorFlow [1d978283-2c37-5f34-9a8e-e9c0ece82495]\n",
      "└ @ Base loading.jl:1186\n",
      "┌ Warning: Loading a new version of TensorFlow.jl for the first time. This initial load can take around 5 minutes as code is precompiled; subsequent usage will only take a few seconds.\n",
      "└ @ TensorFlow ~/.julia/packages/TensorFlow/eu9qM/src/TensorFlow.jl:3\n",
      "┌ Info: Precompiling XLA [1ae4bca4-de81-11e8-0eca-6d3e4e7c4181]\n",
      "└ @ Base loading.jl:1186\n",
      "WARNING: could not import xla.Shape into XLA\n",
      "WARNING: could not import NNlib.cudata into Tracker\n",
      "┌ Info: Precompiling Unrolled [9602ed7d-8fef-5bc8-8597-8f21381861e8]\n",
      "└ @ Base loading.jl:1186\n"
     ]
    }
   ],
   "source": [
    "using TensorFlow, XLA, Flux, Unrolled, Zygote, Printf, Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 4466k  100 4466k    0     0  5775k      0 --:--:-- --:--:-- --:--:-- 5770k\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"shakespeare_input.txt\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "include(joinpath(pathof(XLA), \"../../examples/tpu_optimizers.jl\"))\n",
    "\n",
    "# First, let's download our dataset;\n",
    "if !isfile(\"shakespeare_input.txt\")\n",
    "    download(\"https://cs.stanford.edu/people/karpathy/char-rnn/shakespeare_input.txt\", \"shakespeare_input.txt\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " => Loaded 4573338-character dataset and encoded into 68-symbol embedding\n"
     ]
    }
   ],
   "source": [
    "# Read text in as a giant string, convert to array of characters\n",
    "text = collect(String(read(\"shakespeare_input.txt\")))\n",
    "\n",
    "# Generate alphabet, which we will use as an embedding (along with special \"stop\" character '_')\n",
    "alphabet = sort([unique(text)..., '_'])\n",
    "stop = UInt32(Flux.onehotidx('_', alphabet))\n",
    "\n",
    "# Embed text through alphabet as UInt32 onehot indices\n",
    "text = UInt32.(map(ch -> Flux.onehotidx(ch, alphabet), text))\n",
    "\n",
    "println(\" => Loaded $(length(text))-character dataset and encoded into $(length(alphabet))-symbol embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " => Segmented into 1430 batches of size 64 with 50-element sequences\n"
     ]
    }
   ],
   "source": [
    "# We will process 64 sequences of length 50 at a time.  Reshape `text` into\n",
    "# tensors of shape (seq_len, batch_size, batch_idx).  To reshape cleanly, we\n",
    "# will pad our text with our `stop` character until it is easily reshapable:\n",
    "batch_size = 64\n",
    "seq_len = 50\n",
    "num_batches = ceil(Int, (length(text) - 1)/(seq_len*batch_size))\n",
    "padded_length = seq_len*batch_size*num_batches + 1\n",
    "text = vcat(text, repeat([stop], padded_length - length(text)))\n",
    "\n",
    "# Build Xs and Ys from this text, where each element of `Xs` has its next element\n",
    "# predicted by the corresponding element of `Ys`.\n",
    "Xs = reshape(text[1:end-1], (seq_len, batch_size, num_batches))\n",
    "Ys = reshape(text[2:end-0], (seq_len, batch_size, num_batches))\n",
    "\n",
    "println(\" => Segmented into $(num_batches) batches of size $(batch_size) with $(seq_len)-element sequences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "initialize_state (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_model_state(m::Flux.LSTMCell) = Flux.hidden(m)\n",
    "get_model_state(m::Flux.Recur) = get_model_state(m.cell)\n",
    "function get_model_state(model)\n",
    "    return tuple(\n",
    "        get_model_state(model.layers[1]),\n",
    "        get_model_state(model.layers[2]),\n",
    "    )\n",
    "end\n",
    "\n",
    "# Update LSTM state vectors within a model\n",
    "set_model_state(m::Flux.LSTMCell, state) = Flux.LSTMCell(m.Wi, m.Wh, m.b, state...)\n",
    "set_model_state(m::Flux.Recur, state) = Flux.Recur(set_model_state(m.cell, state))\n",
    "function set_model_state(model::ImmutableChain, state)\n",
    "    return ImmutableChain(\n",
    "        set_model_state(model.layers[1], state[1]),\n",
    "        set_model_state(model.layers[2], state[2]),\n",
    "        model.layers[3],\n",
    "    )\n",
    "end\n",
    "\n",
    "# `Chain` object version, identical except for the `Chain()` constructor name.\n",
    "function set_model_state(model::Chain, state)\n",
    "    return Chain(\n",
    "        set_model_state(model.layers[1], state[1]),\n",
    "        set_model_state(model.layers[2], state[2]),\n",
    "        model.layers[3],\n",
    "    )\n",
    "end\n",
    "\n",
    "# Helper function to \n",
    "function initialize_state(model, x)\n",
    "    # Create zero-vectors of the same length (this disregards batch dimension)\n",
    "    zerovec(h) = Zygote.map(sub_h -> zero(sub_h[:,1]), h)\n",
    "\n",
    "    # Next, run the new x through the cells to broadcast up the dimensions of h1/h2\n",
    "    h1, h2 = zerovec.(get_model_state(model))\n",
    "    h1, x = model.layers[1].cell(h1, x)\n",
    "    h2, x = model.layers[2].cell(h2, x)\n",
    "\n",
    "    # Once we have h1/h2 values of the right shape, create zero matrices of that shape\n",
    "    zeromat(h) = Zygote.map(sub_h -> zero(sub_h), h)\n",
    "    return set_model_state(model, zeromat.((h1, h2)))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImmutableChain{Tuple{Flux.LSTMCell{XRTArray{Float32,(2048, 68),2},XRTArray{Float32,(2048, 512),2},XRTArray{Float32,(2048,),1},XRTArray{Float32,(512, 64),2},XRTArray{Float32,(512, 64),2}},Flux.LSTMCell{XRTArray{Float32,(2048, 512),2},XRTArray{Float32,(2048, 512),2},XRTArray{Float32,(2048,),1},XRTArray{Float32,(512, 64),2},XRTArray{Float32,(512, 64),2}},Dense{typeof(identity),XRTArray{Float32,(68, 512),2},XRTArray{Float32,(68,),1}}}}((LSTMCell(68, 512), LSTMCell(512, 512), Dense(512, 68)))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Chain(\n",
    "    LSTM(length(alphabet), 512),\n",
    "    LSTM(512, 512),\n",
    "    Dense(512, length(alphabet)),\n",
    ")\n",
    "\n",
    "# Initialize model with the proper size, then create tpu_model by mapping CPU version over to TPU constructs\n",
    "model = initialize_state(model, zeros(Float32, length(alphabet), batch_size))\n",
    "tpu_model = map_to_tpu(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "full_model_unrolled_expansion_ (generic function with 1 method)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Push `x` through, updating our state.  Note that normally, with `Flux`, we would\n",
    "# just call `model(x)`, but because we don't want `Flux` to keep track of the\n",
    "# internal layer state, we instead manually call the layer methods, passing the\n",
    "# state vectors in explicitly.\n",
    "function single_model_timestep(model, state, x)\n",
    "    # Unpack state for our LSTM layers\n",
    "    h1, h2 = state\n",
    "    \n",
    "    # First, the two LSTM layers\n",
    "    h1, x = model.layers[1](h1, x)\n",
    "    h2, x = model.layers[2](h2, x)\n",
    "    \n",
    "    # Next, the Dense layer\n",
    "    y_hat = model.layers[3](x)\n",
    "\n",
    "    # Return y_hat and our state\n",
    "    return y_hat, (h1, h2)\n",
    "end\n",
    "\n",
    "\n",
    "# Helper function to convert a batch of text at a particular time point into first a OneHotMatrix,\n",
    "# and then densifying that OneHotMatrix into a typical XRTArray{Float32} which we can apply\n",
    "# logitcrossentropy loss upon.\n",
    "function densify(::Val{alphabet_size}, x::XRTArray, t) where {alphabet_size}\n",
    "    return convert(XRTArray{Float32}, Flux.OneHotMatrix(alphabet_size, x[XRTArray(t), :]))\n",
    "end\n",
    "function densify(::Val{alphabet_size}, x, t) where {alphabet_size}\n",
    "    return Flux.OneHotMatrix(alphabet_size, x[t, :])\n",
    "end\n",
    "\n",
    "# This function runs the full model on a batch of `x` and `y` data, returning the resultant loss\n",
    "# to be minimized.  Note that we must unroll the internal for loop so that our automatic\n",
    "# differentiation package Zygote doesn't try to dynamically create stacks to track the changing\n",
    "# variable values through iterations across time.\n",
    "@unroll function full_model(unused::Val{alphabet_size}, model, x_batch::XRTArray, y_batch::XRTArray) where {alphabet_size}\n",
    "    # Get current LSTM state\n",
    "    state = get_model_state(model)\n",
    "    \n",
    "    # Accumulate loss into here\n",
    "    loss = XRTArray(0f0)\n",
    "\n",
    "    # Iterate over time\n",
    "    @unroll for time_idx = 1:size(x_batch, 1)\n",
    "        # Create dense representations of the one-hot encoded text at this point in time, across an entire batch\n",
    "        x = densify(Val(alphabet_size), x_batch, time_idx)\n",
    "        \n",
    "        # Push x through our model to get y_hat (and new recurrent state values)\n",
    "        y_hat, state = single_model_timestep(model, state, x)\n",
    "        \n",
    "        # Accumulate loss\n",
    "        loss += crossentropy(softmax(y_hat), densify(Val(alphabet_size), y_batch, time_idx))\n",
    "    end\n",
    "\n",
    "    # Return loss and updated model\n",
    "    return loss, model\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_model (generic function with 1 method)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function train_model(::Val{alphabet_size}, ::Val{num_epochs}, model, Xs, Ys, η) where {alphabet_size, num_epochs}\n",
    "    # Create optimizer, in this case ADAM, as gradient scaling is very important for RNNs.\n",
    "    opt = TPU_ADAM(model, η, (XRTArray(0.9f0), XRTArray(0.999f0)))\n",
    "    \n",
    "    # We will report loss once every epoch, store it here in the meantime, one loss value\n",
    "    # per minibatch, spat out over the wire at the end of every epoch.\n",
    "    loss_buffer = zero(XRTArray{Float32, (size(Xs, 3),), 1})\n",
    "\n",
    "    # Iterate over epochs\n",
    "    epoch_idx = XRTArray(1)\n",
    "    while epoch_idx <= XRTArray(num_epochs)\n",
    "        # We need to randomly shuffle our dataset every epoch, both for good convergence\n",
    "        # properties, and also so that different TPUs are processing different batches\n",
    "        # in parallel.  To do so, we just randomize the order in which we visit batches\n",
    "        # in our dataset (our dataset is small enough that it fits on TPU memory no\n",
    "        # problem).  We do not shuffle the contents of batches, only the order in which\n",
    "        # we visit the predetermined minibatches.\n",
    "        batch_permutation = XLA.shuffle(XRTArray(1:size(Xs, 3)))\n",
    "        \n",
    "        # Iterate over batches within an epoch\n",
    "        batch_idx = XRTArray(1)\n",
    "        while batch_idx <= XRTArray(size(Xs, 3))\n",
    "            # Calculate forward pass of model, and compile backward pass stored in `back()`.\n",
    "            # Use `let` block to work around Julia inference limitations.  The forward pass\n",
    "            # returns a new model, because it will have updated the internal state vectors.\n",
    "            (loss, model), back = let model=model,\n",
    "                                      x_batch=Xs[:, :, batch_permutation[batch_idx]],\n",
    "                                      y_batch=Ys[:, :, batch_permutation[batch_idx]]\n",
    "                Zygote._forward(\n",
    "                    Zygote.Context{Nothing}(nothing),\n",
    "                    model -> full_model(Val(alphabet_size), model, x_batch, y_batch),\n",
    "                    model,\n",
    "                )\n",
    "            end\n",
    "            \n",
    "            # Invoke `back()` with sensitivity `1f0` on the `loss`, and `nothing` on the\n",
    "            # `model` output of `full_model()`.\n",
    "            Δ_model = Zygote.tailmemaybe(back((1f0, nothing)))[1]\n",
    "\n",
    "            # Cross-replica sum our model updates to average across all tpus\n",
    "            Δ_model = XLA.unflatten_tuple(Δ_model,\n",
    "               XLA.HloCrossReplicaSum{typeof(+)}((), 0, \"\")(\n",
    "                   +,\n",
    "                   XLA.flatten_tuple(Δ_model)...\n",
    "               )\n",
    "            )\n",
    "\n",
    "            # Update parameters via our optimizer\n",
    "            opt, model = update!(opt, model, Δ_model)\n",
    "\n",
    "            # Store loss over an epoch into `loss_buffer`.\n",
    "            loss_buffer = Base.setindex(loss_buffer, loss, batch_idx)\n",
    "\n",
    "            # Increment batch_idx\n",
    "            batch_idx += XRTArray(1)\n",
    "        end\n",
    "        \n",
    "        # Once per epoch, output our training loss for the entire epoch\n",
    "        XLA.HloOutfeed()((loss_buffer,), XLA.HloAfterAll()())\n",
    "        \n",
    "        # Increment epoch_idx\n",
    "        epoch_idx += XRTArray(1)\n",
    "    end\n",
    "    \n",
    "    # Return the trained model (note that this gets returned from each of our TPUs, but we\n",
    "    # only pay attention to the model returned from the first node, since they are all\n",
    "    # identical thanks to the cross-replica sum above in the training loop)\n",
    "    return model\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to TPU on 10.240.1.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-15 02:19:05.710694: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:349] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TPUSession(<8 TPU chips in 2x2x2 topology>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tpu_ip = \"10.240.1.2\"\n",
    "println(\"Connecting to TPU on $(tpu_ip)\")\n",
    "XLA.initialize!(\"$(tpu_ip):8470\"; reset=true)\n",
    "\n",
    "sess = XLA.global_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Compilation failed; attempting to explain suboptimal inference:\n",
      "└ @ Main /home/wkh/XLA.jl/src/compiler_interface.jl:121\n"
     ]
    },
    {
     "ename": "TensorFlow.TFException",
     "evalue": "Tensorflow error: Status: While rewriting computation to not contain X64 element types, XLA encountered an HLO for which this rewriting is not implemented: %comp3_sort102 = (u32[1430]{0}, s64[1430]{0}) sort(u32[1430]{0} %comp3_rng101, s64[1430]{0} %comp3_get-tuple-element98), dimensions={0}, sharding={{maximal device=0}, {maximal device=0}}\n\t [[{{node XRTCompile}} = XRTCompile[_device=\"/job:tpu_worker/replica:0/task:0/device:TPU:0\"](_recv_placeholder_0_G4)]]\n",
     "output_type": "error",
     "traceback": [
      "Tensorflow error: Status: While rewriting computation to not contain X64 element types, XLA encountered an HLO for which this rewriting is not implemented: %comp3_sort102 = (u32[1430]{0}, s64[1430]{0}) sort(u32[1430]{0} %comp3_rng101, s64[1430]{0} %comp3_get-tuple-element98), dimensions={0}, sharding={{maximal device=0}, {maximal device=0}}\n\t [[{{node XRTCompile}} = XRTCompile[_device=\"/job:tpu_worker/replica:0/task:0/device:TPU:0\"](_recv_placeholder_0_G4)]]\n",
      "",
      "Stacktrace:",
      " [1] check_status at /home/wkh/.julia/packages/TensorFlow/eu9qM/src/core.jl:374 [inlined]",
      " [2] #run#60(::Bool, ::Function, ::Session, ::Array{TensorFlow.Port,1}, ::Array{Any,1}, ::Array{TensorFlow.Port,1}, ::Array{Ptr{Nothing},1}) at /home/wkh/.julia/packages/TensorFlow/eu9qM/src/run.jl:117",
      " [3] run at /home/wkh/.julia/packages/TensorFlow/eu9qM/src/run.jl:90 [inlined]",
      " [4] #run#69(::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}, ::Function, ::Session, ::Array{Tensor{Int64},1}, ::Dict{Tensor{String},String}) at /home/wkh/.julia/packages/TensorFlow/eu9qM/src/run.jl:233",
      " [5] run at /home/wkh/.julia/packages/TensorFlow/eu9qM/src/run.jl:210 [inlined]",
      " [6] #run#70(::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}, ::Function, ::Session, ::Tensor{Int64}, ::Dict{Tensor{String},String}) at /home/wkh/.julia/packages/TensorFlow/eu9qM/src/run.jl:254",
      " [7] run(::Session, ::Tensor{Int64}, ::Dict{Tensor{String},String}) at /home/wkh/.julia/packages/TensorFlow/eu9qM/src/run.jl:254",
      " [8] compile(::TPUSession, ::XLA.xrt.XLAComputation, ::Type) at /home/wkh/XLA.jl/src/xrt.jl:44",
      " [9] top-level scope at /home/wkh/XLA.jl/src/compiler_interface.jl:119",
      " [10] top-level scope at In[14]:8"
     ]
    }
   ],
   "source": [
    "# Note: This may take a few minutes (will improve in the future)\n",
    "num_epochs = 10\n",
    "η = 0.001f0\n",
    "\n",
    "# Compile the model\n",
    "t_start = time()\n",
    "all_tpus = all_tpu_devices(sess)\n",
    "compilation_handle = @tpu_compile devices=all_tpus train_model(Val(length(alphabet)), Val(num_epochs), tpu_model, XRTArray(Xs), XRTArray(Ys), XRTArray(0.01f0));\n",
    "t_end = time()\n",
    "\n",
    "println(@sprintf(\"=> Compiled training loop in %.1f seconds\", t_end - t_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: compilation_handle not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: compilation_handle not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[15]:2"
     ]
    }
   ],
   "source": [
    "t_start = time()\n",
    "loop_task = XLA.run_on_devices(compilation_handle, tpu_model, Xs, Ys, η)\n",
    "t_end = time()\n",
    "\n",
    "println(@sprintf(\"=> Launched training loop on %d TPUs in %.1f seconds\", length(all_tpus), t_end - t_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.1.0-DEV",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
